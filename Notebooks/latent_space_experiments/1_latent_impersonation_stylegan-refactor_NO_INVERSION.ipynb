{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bedcd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Latent-Space Face Impersonation (Reduced-Compute Variant)\n",
    "# MSc Dissertation - Stella Williams\n",
    "#\n",
    "# This notebook implements an inversion-free latent-space\n",
    "# impersonation experiment using:\n",
    "#   - StyleGAN2-ADA (generator)\n",
    "#   - FaceNet (verification backend)\n",
    "#\n",
    "# Objective:\n",
    "# Generate a synthetic face whose embedding matches a selected\n",
    "# target identity embedding, without reconstructing a source face.\n",
    "#\n",
    "# This models a target-synthesis threat scenario rather than\n",
    "# source-to-target inversion.\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e011c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39a645b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"Cell 2: Device, FaceNet setup\"\n",
    "# --------------------------------------------------------------\n",
    "# Device configuration\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# FaceNet embedding model (verification system)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "facenet = InceptionResnetV1(\n",
    "    pretrained=\"vggface2\"\n",
    ").eval().to(device)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Preprocessing for FaceNet (160x160 input requirement)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "to_facenet = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def img_to_tensor(img):\n",
    "    return to_facenet(img).unsqueeze(0).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def facenet_embed(img_tensor):\n",
    "    emb = facenet(img_tensor)\n",
    "    return nn.functional.normalize(emb, p=2, dim=1)\n",
    "\n",
    "def cosine(a, b):\n",
    "    return (a * b).sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65b1dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded StyleGAN2 generator\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Load pretrained StyleGAN2-ADA generator\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "STYLEGAN_PKL = \"/Users/stel/Documents/Dissertation/msc-biometric-security-clean/Models/stylegan2-ffhq.pkl\"\n",
    "\n",
    "with open(STYLEGAN_PKL, \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].to(device).eval()\n",
    "\n",
    "print(\"Loaded StyleGAN2 generator\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c41ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Generator wrapper\n",
    "# Converts single w latent into w+ internally\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def synth_from_w_single(w):\n",
    "    \"\"\"\n",
    "    w: [1, w_dim]\n",
    "    Broadcast internally to w+ representation.\n",
    "    Returns image in [0,1].\n",
    "    \"\"\"\n",
    "    w_plus = w.unsqueeze(1).repeat(1, G.synthesis.num_ws, 1)\n",
    "    img = G.synthesis(w_plus, noise_mode=\"const\")\n",
    "    img = (img + 1) / 2\n",
    "    return img.clamp(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d719d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial latent shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Inversion-free initialisation\n",
    "# Use average latent as starting point\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "with torch.no_grad():\n",
    "    w = G.mapping.w_avg.unsqueeze(0).to(device)\n",
    "\n",
    "w = w.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(\"Initial latent shape:\", w.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8a4a9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target embedding norm: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Load target identity (impersonation objective)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "TARGET_A_PATH = \"/Users/stel/Documents/Dissertation/msc-biometric-security-clean/Datasets/lfw/Lenny_Kravitz/Lenny_Kravitz_0001.jpg\"\n",
    "\n",
    "target_img = load_image(TARGET_A_PATH)\n",
    "target_t = img_to_tensor(target_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    e_target = facenet_embed(target_t)\n",
    "\n",
    "print(\"Target embedding norm:\", e_target.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19291a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 000 | cosine similarity = -0.0156\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Reduced-compute latent impersonation loop\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "optimizer = optim.Adam([w], lr=0.03)\n",
    "\n",
    "MAX_STEPS = 80\n",
    "TARGET_SIM = 0.85   # approximate verification threshold\n",
    "\n",
    "for step in range(MAX_STEPS):\n",
    "    \n",
    "    # Generate synthetic face\n",
    "    img = synth_from_w_single(w)\n",
    "    img_160 = nn.functional.interpolate(img, (160, 160))\n",
    "    \n",
    "    # Compute embedding\n",
    "    emb = facenet(img_160)\n",
    "    emb = nn.functional.normalize(emb, dim=1)\n",
    "    \n",
    "    # Targeted impersonation objective\n",
    "    sim = cosine(emb, e_target)\n",
    "    loss = 1 - sim.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:03d} | cosine similarity = {sim.item():.4f}\")\n",
    "    \n",
    "    # Early stopping if threshold reached\n",
    "    if sim.item() >= TARGET_SIM:\n",
    "        print(\"Early stop: impersonation threshold reached\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789472c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Final evaluation\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_img = synth_from_w_single(w)\n",
    "    final_160 = nn.functional.interpolate(final_img, (160,160))\n",
    "    emb_final = facenet_embed(final_160)\n",
    "\n",
    "final_similarity = cosine(emb_final, e_target).item()\n",
    "\n",
    "print(\"Final cosine similarity to target:\", final_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ccaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Visualise final generated adversarial face\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(final_img.squeeze().permute(1,2,0).cpu())\n",
    "plt.title(f\"Latent-space impersonation | cos={final_similarity:.3f}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
