{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bedcd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Latent-Space Face Impersonation (Reduced-Compute Variant)\n",
    "# MSc Dissertation - Stella Williams\n",
    "# Inversion-free latent target synthesis experiment\n",
    "# Multi-run evaluation for repeatability and target specificity\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e011c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39a645b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"Cell 2: Device, FaceNet setup\"\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Device configuration\n",
    "# --------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# FaceNet embedding model (verification system)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "facenet = InceptionResnetV1(\n",
    "    pretrained=\"vggface2\"\n",
    ").eval().to(device)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Preprocessing for FaceNet (160x160 input requirement)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "to_facenet = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def img_to_tensor(img):\n",
    "    return to_facenet(img).unsqueeze(0).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def facenet_embed(img_tensor):\n",
    "    emb = facenet(img_tensor)\n",
    "    return nn.functional.normalize(emb, p=2, dim=1)\n",
    "\n",
    "def cosine(a, b):\n",
    "    return (a * b).sum(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65b1dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded StyleGAN2 generator\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Load pretrained StyleGAN2-ADA generator\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "sys.path.append(\"/Users/stel/Documents/Dissertation/msc-biometric-security-clean/stylegan2-ada-pytorch\")\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "STYLEGAN_PKL = \"/Users/stel/Documents/Dissertation/msc-biometric-security-clean/Models/stylegan2-ffhq.pkl\"\n",
    "\n",
    "with open(STYLEGAN_PKL, \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].to(device).eval()\n",
    "\n",
    "print(\"Loaded StyleGAN2 generator\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c41ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Generator wrapper\n",
    "# Converts single w latent into w+ internally\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def synth_from_w_single(w):\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "    - w shape [1, w_dim]\n",
    "    - w shape [1, num_ws, w_dim] (w+)\n",
    "\n",
    "    Returns image in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    if w.ndim == 2:\n",
    "        # w is [1, w_dim] â†’ broadcast to w+\n",
    "        w_plus = w.unsqueeze(1).repeat(1, G.synthesis.num_ws, 1)\n",
    "    elif w.ndim == 3:\n",
    "        # Already w+\n",
    "        w_plus = w\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected latent shape: {w.shape}\")\n",
    "\n",
    "    img = G.synthesis(w_plus, noise_mode=\"const\")\n",
    "    img = (img + 1) / 2\n",
    "    return img.clamp(0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce09a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target embedding norm: 1.0\n",
      "Initial w shape: torch.Size([1, 18, 512])\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Load target identity (impersonation objective)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "TARGET_A_PATH = \"/Users/stel/Documents/Dissertation/msc-biometric-security-clean/Datasets/lfw/Lenny_Kravitz/Lenny_Kravitz_0001.jpg\"\n",
    "TARGET_B_PATH = \"/Users/stel/Documents/Dissertation/msc-biometric-security-clean/Datasets/lfw/Patricia_Clarkson/Patricia_Clarkson_0001.jpg\"\n",
    "\n",
    "targetA_img = load_image(TARGET_A_PATH)\n",
    "targetB_img = load_image(TARGET_B_PATH)\n",
    "\n",
    "targetA_t = img_to_tensor(targetA_img)\n",
    "targetB_t = img_to_tensor(targetB_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    e_target = facenet_embed(targetA_t)\n",
    "    e_non_target = facenet_embed(targetB_t)\n",
    "\n",
    "print(\"Target embedding norm:\", e_target.norm().item())\n",
    "\n",
    "print(\"Initial w shape:\", w.shape)\n",
    "#inspecting the shape, it is likely [1, 1, w_dim], or [1, num_ws, w_dim] - with the latter meaning\n",
    "#we are alreadz in w+ space and the wrapper must not broadcast again. in my previous version, no loop,\n",
    "#i initialised w differently, mapping for from a random latent or mapping. dimensions changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b3962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running latent impersonation batch...\n",
      "\n",
      "\n",
      "=== Run 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Run 1:   0%|                                             | 0/80 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Multi-Run Latent Impersonation Experiment   \n",
    "# --------------------------------------------------------------    \n",
    "NUM_RUNS = 5\n",
    "MAX_STEPS = 80\n",
    "TARGET_SIM = 0.85\n",
    "LR = 0.03\n",
    "\n",
    "results = []\n",
    "best_example = None\n",
    "\n",
    "print(\"\\nRunning latent impersonation batch...\\n\")\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "\n",
    "    print(f\"\\n=== Run {run+1}/{NUM_RUNS} ===\")\n",
    "\n",
    "    # Random latent initialisation\n",
    "    z = torch.randn(1, G.z_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        w = G.mapping(z, None)\n",
    "\n",
    "    w = w.clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer = optim.Adam([w], lr=LR)\n",
    "\n",
    "    start_time = time.time()\n",
    "    success = False\n",
    "\n",
    "    for step in tqdm(range(MAX_STEPS), desc=f\"Run {run+1}\", leave=False):\n",
    "\n",
    "        img = synth_from_w_single(w)\n",
    "\n",
    "        img_160 = nn.functional.interpolate(img, (160,160))\n",
    "\n",
    "        emb = facenet(img_160)\n",
    "        emb = nn.functional.normalize(emb, dim=1)\n",
    "\n",
    "        sim_target = cosine(emb, e_target)\n",
    "        loss = 1 - sim_target.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if sim_target.item() >= TARGET_SIM:\n",
    "            success = True\n",
    "            break\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_img = synth_from_w_single(w)\n",
    "        final_160 = nn.functional.interpolate(final_img, (160,160))\n",
    "        emb_final = facenet_embed(final_160)\n",
    "\n",
    "        final_sim_target = cosine(emb_final, e_target).item()\n",
    "        final_sim_non_target = cosine(emb_final, e_non_target).item()\n",
    "\n",
    "    results.append({\n",
    "        \"success\": success,\n",
    "        \"steps\": step + 1,\n",
    "        \"final_sim_target\": final_sim_target,\n",
    "        \"final_sim_non_target\": final_sim_non_target,\n",
    "        \"runtime_sec\": runtime\n",
    "    })\n",
    "\n",
    "    if success and best_example is None:\n",
    "        best_example = final_img.detach().cpu()\n",
    "\n",
    "    print(f\"Success: {success}\")\n",
    "    print(f\"Steps: {step+1}\")\n",
    "    print(f\"Final cosine (target): {final_sim_target:.4f}\")\n",
    "    print(f\"Final cosine (non-target): {final_sim_non_target:.4f}\")\n",
    "    print(f\"Runtime: {runtime/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Summary statistics\n",
    "# ----------------------------------\n",
    "\n",
    "success_rate = sum(r[\"success\"] for r in results) / NUM_RUNS\n",
    "mean_steps = np.mean([r[\"steps\"] for r in results])\n",
    "mean_cos = np.mean([r[\"final_sim_target\"] for r in results])\n",
    "mean_non_target = np.mean([r[\"final_sim_non_target\"] for r in results])\n",
    "mean_runtime = np.mean([r[\"runtime_sec\"] for r in results])\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Batch Summary\")\n",
    "print(\"==============================\")\n",
    "print(f\"Success rate: {success_rate*100:.1f}%\")\n",
    "print(f\"Mean steps: {mean_steps:.2f}\")\n",
    "print(f\"Mean final cosine (target): {mean_cos:.4f}\")\n",
    "print(f\"Mean cosine (non-target): {mean_non_target:.4f}\")\n",
    "print(f\"Mean runtime: {mean_runtime/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Visualise final generated adversarial face\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "if best_example is not None:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(best_example.squeeze().permute(1,2,0))\n",
    "    plt.title(\"Example successful latent impersonation\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
